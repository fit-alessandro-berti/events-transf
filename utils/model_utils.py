# File: utils/model_utils.py
import torch
import os
import re

# --- Import from project files ---
from config import CONFIG
from data_generator import XESLogLoader
# --- ğŸ”» MODIFIED IMPORTS ğŸ”» ---
from components.meta_learner import MetaLearner
from components.moe_model import MoEModel  # Import the new MoE wrapper


# --- ğŸ”º END MODIFIED ğŸ”º ---


def init_loader(config):
    """Initializes and returns an XESLogLoader based on the config."""
    strategy = config['embedding_strategy']
    sbert_model_name = config['pretrained_settings']['sbert_model']
    loader = XESLogLoader(strategy=strategy, sbert_model_name=sbert_model_name)
    return loader


def create_model(config, loader, device):
    """
    Initializes the MoEModel (which wraps MetaLearner(s))
    based on the config and loader.
    """
    strategy = config['embedding_strategy']

    # --- ğŸ”» MoE Config ğŸ”» ---
    moe_config = config.get('moe_settings', {})
    num_experts = moe_config.get('num_experts', 1)
    # --- ğŸ”º End MoE Config ğŸ”º ---

    if strategy == 'pretrained':
        model_params = {'embedding_dim': config['pretrained_settings']['embedding_dim']}
    else:  # learned
        # Ensure loader is fitted or artifacts loaded to get char_vocab_size
        if not loader.char_to_id:
            raise RuntimeError("Loader must be fitted or artifacts loaded before creating 'learned' model.")
        model_params = {
            'char_vocab_size': len(loader.char_to_id),
            'char_embedding_dim': config['learned_settings']['char_embedding_dim'],
            'char_cnn_output_dim': config['learned_settings']['char_cnn_output_dim'],
        }

    # --- ğŸ”» MODIFIED: Instantiate MoEModel ğŸ”» ---
    # This will create MetaLearner(s) internally
    model = MoEModel(
        num_experts=num_experts,
        strategy=strategy,
        num_feat_dim=config['num_numerical_features'],
        d_model=config['d_model'],
        n_heads=config['n_heads'],
        n_layers=config['n_layers'],
        dropout=config['dropout'],
        **model_params
    ).to(device)
    # --- ğŸ”º END MODIFIED ğŸ”º ---

    # Pass the character vocabulary to the model
    # The MoEModel will pass it down to all its experts
    if strategy == 'learned':
        model.set_char_vocab(loader.char_to_id)

    return model


def load_model_weights(model, checkpoint_dir, device):
    """Finds the latest checkpoint and loads its weights into the model."""
    if not os.path.isdir(checkpoint_dir):
        exit("âŒ Error: Checkpoint directory not found.")

    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith('model_epoch_') and f.endswith('.pth')]
    if not checkpoints:
        exit("âŒ Error: No model checkpoints found.")

    latest_checkpoint_path = os.path.join(
        checkpoint_dir,
        sorted(checkpoints, key=lambda f: int(re.search(r'(\d+)', f).group(1)))[-1]
    )
    print(f"ğŸ” Found latest checkpoint: {os.path.basename(latest_checkpoint_path)}")

    print(f"ğŸ’¾ Loading weights from {latest_checkpoint_path}...")
    model.load_state_dict(torch.load(latest_checkpoint_path, map_location=device))
    return latest_checkpoint_path
