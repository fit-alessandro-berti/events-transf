# File: evaluation/eval_retrieval.py
import torch
import torch.nn.functional as F
import random
import numpy as np
from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score
from tqdm import tqdm

# --- Import from project files ---
from time_transf import inverse_transform_time
# ðŸ”» NEW IMPORT ðŸ”»
from utils.retrieval_utils import find_knn_indices


# ðŸ”º END NEW ðŸ”º


def _get_all_test_embeddings(model, test_tasks_list, batch_size=64):
    """
    Helper function to compute embeddings for all (prefix, label, case_id) tuples.
    (Moved from testing.py)

    *** ASSUMPTION ***: This function assumes test_tasks_list contains tuples of
    (prefix, label, case_id) as generated by a modified get_task_data function.

    MODIFIED for MoE:
    - Calls `model._process_batch`, which in `MoEModel` will return
      the *average* embedding from all experts.
    """
    all_embeddings = []
    all_labels = []
    all_case_ids = []  # <-- For contamination fix

    device = next(model.parameters()).device
    model.eval()

    try:
        # Check if data has 3 items (prefix, label, case_id)
        _ = test_tasks_list[0][2]
    except (IndexError, TypeError):
        print("\n" + "=" * 50)
        print("âŒ ERROR in _get_all_test_embeddings:")
        print("Test data does not contain case_ids.")
        print("Please modify get_task_data in data_generator.py to return:")
        print("(prefix, label, case_id) tuples.")
        print("Aborting retrieval-augmented evaluation.")
        print("=" * 50 + "\n")
        return None, None, None

    with torch.no_grad():
        for i in tqdm(range(0, len(test_tasks_list), batch_size), desc="Pre-computing test embeddings"):
            batch_tasks = test_tasks_list[i:i + batch_size]
            sequences = [t[0] for t in batch_tasks]
            labels = [t[1] for t in batch_tasks]
            case_ids = [t[2] for t in batch_tasks]  # <-- Get case_ids

            if not sequences: continue

            # Use the model's internal processing function
            # For MoEModel, this returns the average embedding
            encoded_batch = model._process_batch(sequences)

            all_embeddings.append(encoded_batch.cpu())
            all_labels.extend(labels)
            all_case_ids.extend(case_ids)  # <-- Store case_ids

    if not all_embeddings:
        return None, None, None

    all_embeddings_tensor = torch.cat(all_embeddings, dim=0).to(device)
    all_labels_tensor = torch.as_tensor(all_labels, device=device)
    all_case_ids_array = np.array(all_case_ids)  # Use numpy for easy string comparison

    return all_embeddings_tensor, all_labels_tensor, all_case_ids_array


def evaluate_retrieval_augmented(
    model,
    test_tasks,
    num_retrieval_k_list,
    num_test_queries=200,
    candidate_percentages=None
):
    """
    Retrieval-Augmented evaluation.
    (Moved from testing.py)

    1. Computes all test embeddings (uses avg. expert embedding for MoE).
    2. For each query, finds k-NN *from other cases* to form the support set.
    3. Uses the proto_head from the *first expert* to make predictions.

    candidate_percentages: list of percentages of the candidate pool to sample
    for k-NN (100 = full pool).
    """
    print("\nðŸ”¬ Starting Retrieval-Augmented Evaluation...")
    model.eval()

    if not candidate_percentages:
        candidate_percentages = [100]

    # --- ðŸ”» MoE Fix ðŸ”» ---
    # The retrieval eval is tightly coupled to the MetaLearner's proto_head.
    # For MoE, we'll use the *average embedding space* (from _get_all_test_embeddings)
    # and the *first expert's* proto_head for making the final k-NN prediction.
    # This is an approximation but maintains compatibility.
    proto_head_to_use = model.experts[0].proto_head
    if model.num_experts > 1:
        print("  - (MoE) Using avg. embedding space and Expert 0's proto_head for k-NN eval.")
    # --- ðŸ”º End MoE Fix ðŸ”º ---

    task_embeddings = {}

    # --- 1. Pre-compute all embeddings ---
    for task_type, task_data in test_tasks.items():
        if not task_data:
            print(f"Skipping {task_type}: No test data available.")
            continue

        # Get embeddings, labels, and case_ids
        embeddings, labels, case_ids = _get_all_test_embeddings(model, task_data)

        if embeddings is None:  # Error already printed in helper
            return

        # L2-normalize for efficient cosine similarity
        embeddings = F.normalize(embeddings, p=2, dim=1)
        task_embeddings[task_type] = (embeddings, labels, case_ids)  # Store all 3
        print(f"  - Pre-computed {embeddings.shape[0]} embeddings for {task_type}.")

    # --- 2. Evaluate using k-NN retrieval ---
    for task_type, (all_embeddings, all_labels, all_case_ids) in task_embeddings.items():
        print(f"\n--- Evaluating task: {task_type} ---")

        num_total_samples = all_embeddings.shape[0]
        if num_total_samples < 2:
            print("Skipping: Not enough samples to evaluate.")
            continue

        num_queries = min(num_test_queries, num_total_samples)
        query_indices = random.sample(range(num_total_samples), num_queries)

        for pct in candidate_percentages:
            print(f"\n  - Candidate pool sampling: {pct}%")

            # Sample a global candidate pool once per percentage for consistent k comparisons
            if pct >= 100:
                non_candidate_indices_tensor = None
            elif pct <= 0:
                non_candidate_indices_tensor = torch.arange(
                    num_total_samples,
                    device=all_embeddings.device,
                    dtype=torch.long
                )
            else:
                sample_size = int(np.ceil(num_total_samples * (pct / 100.0)))
                sample_size = max(1, min(sample_size, num_total_samples))
                if sample_size == num_total_samples:
                    non_candidate_indices_tensor = None
                else:
                    candidate_pool_indices = np.random.choice(
                        num_total_samples,
                        size=sample_size,
                        replace=False
                    )
                    mask_np = np.ones(num_total_samples, dtype=bool)
                    mask_np[candidate_pool_indices] = False
                    non_candidate_indices_tensor = torch.from_numpy(
                        np.where(mask_np)[0]
                    ).to(all_embeddings.device)

            for k in num_retrieval_k_list:
                # Need at least k+1 samples (1 query, k support) *from different cases*
                # This check is complex, so we'll just check if k < N
                if k >= num_total_samples:
                    print(f"Skipping [k={k} | pct={pct}%]: k is larger than total samples.")
                    continue

                # ðŸ”» MODIFIED: Store preds and confidences ðŸ”»
                all_preds, all_true_labels, all_confidences = [], [], []
                # ðŸ”º END MODIFIED ðŸ”º

                for query_idx in query_indices:
                    query_embedding = all_embeddings[query_idx:query_idx + 1]  # [1, D]
                    query_label = all_labels[query_idx]

                    query_case_id = all_case_ids[query_idx]
                    same_case_indices_np = np.where(all_case_ids == query_case_id)[0]

                    if non_candidate_indices_tensor is None:
                        if same_case_indices_np.size == 0:
                            mask_tensor = None
                        else:
                            mask_tensor = torch.from_numpy(same_case_indices_np).to(all_embeddings.device)
                    else:
                        if same_case_indices_np.size == 0:
                            mask_tensor = non_candidate_indices_tensor
                        else:
                            same_case_tensor = torch.from_numpy(same_case_indices_np).to(all_embeddings.device)
                            mask_tensor = torch.cat([non_candidate_indices_tensor, same_case_tensor])

                    # Use the new helper function
                    # Note: query_embedding and all_embeddings are already normalized
                    top_k_indices = find_knn_indices(
                        query_embedding,
                        all_embeddings,
                        k=k,
                        indices_to_mask=mask_tensor
                    )

                    if top_k_indices.numel() == 0:
                        continue  # Not enough valid support items found

                    # ðŸ”ºðŸ”ºðŸ”º END REFACTORED BLOCK ðŸ”ºðŸ”ºðŸ”º

                    support_embeddings = all_embeddings[top_k_indices]  # [k, D]
                    support_labels = all_labels[top_k_indices]  # [k]

                    # --- Get Prediction ---
                    with torch.no_grad():
                        if task_type == 'classification':
                            # --- ðŸ”» MoE Fix ðŸ”» ---
                            logits, proto_classes, confidence = proto_head_to_use.forward_classification(
                                support_embeddings, support_labels, query_embedding
                            )
                            # --- ðŸ”º End MoE Fix ðŸ”º ---

                            if logits is None: continue  # Should not happen if top_k_indices.numel() > 0

                            # --- "NO INVALID QUERIES" FIX ---
                            # Get the index of the top logit (e.g., 0, 1, 2...)
                            pred_label_idx = torch.argmax(logits, dim=1).item()

                            # ðŸ”» MODIFIED: Get confidence for the predicted class ðŸ”»
                            # Index [0, pred_label_idx] to handle [1,C] shape
                            pred_confidence = confidence[0, pred_label_idx].item()
                            # ðŸ”º END MODIFIED ðŸ”º

                            # Find the *actual* class label (e.g., "Activity C")
                            # that corresponds to this index
                            predicted_class_label = proto_classes[pred_label_idx].item()

                            all_preds.append(predicted_class_label)
                            all_true_labels.append(query_label.item())
                            # ðŸ”» MODIFIED: Store confidence ðŸ”»
                            all_confidences.append(pred_confidence)
                            # ðŸ”º END MODIFIED ðŸ”º
                            # --- END FIX ---

                        else:  # Regression
                            # --- ðŸ”» MoE Fix ðŸ”» ---
                            prediction, confidence = proto_head_to_use.forward_regression(
                                support_embeddings, support_labels.float(), query_embedding
                            )
                            # --- ðŸ”º End MoE Fix ðŸ”º ---

                            # ðŸ”» MODIFIED: Index [0] for single-item tensors ðŸ”»
                            all_preds.append(prediction[0].item())
                            all_true_labels.append(query_label.item())
                            # ðŸ”» MODIFIED: Store confidence ðŸ”»
                            all_confidences.append(confidence[0].item())
                            # ðŸ”º END MODIFIED ðŸ”º

                if not all_true_labels:
                    print(f"Skipping [k={k} | pct={pct}%]: no valid queries.")
                    continue

                # --- 3. Report Metrics ---
                if task_type == 'classification':
                    # ðŸ”» MODIFIED: Report confidence ðŸ”»
                    avg_conf = np.mean(all_confidences)
                    print(
                        f"[{k}-NN | pct={pct}%] Retrieval Accuracy: {accuracy_score(all_true_labels, all_preds):.4f} | Avg. Confidence: {avg_conf:.4f} (on {len(all_true_labels)} queries)")
                    # ðŸ”º END MODIFIED ðŸ”º
                else:
                    # ðŸ”» MODIFIED: Report confidence ðŸ”»
                    preds_np = np.array(all_preds)
                    labels_np = np.array(all_true_labels)
                    avg_conf = np.mean(all_confidences)

                    preds = inverse_transform_time(preds_np);
                    preds[preds < 0] = 0
                    labels = inverse_transform_time(labels_np)
                    print(
                        f"[{k}-NN | pct={pct}%] Retrieval MAE: {mean_absolute_error(labels, preds):.4f} | R-squared: {r2_score(labels, preds):.4f} | Avg. Confidence: {avg_conf:.4f}")
                    # ðŸ”º END MODIFIED ðŸ”º
