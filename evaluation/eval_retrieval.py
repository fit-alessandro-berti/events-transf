# File: evaluation/eval_retrieval.py
import torch
import torch.nn.functional as F
import random
import numpy as np
from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score
from tqdm import tqdm

# --- Import from project files ---
from time_transf import inverse_transform_time


def _get_all_test_embeddings(model, test_tasks_list, batch_size=64):
    """
    Helper function to compute embeddings for all (prefix, label, case_id) tuples.
    (Moved from testing.py)

    *** ASSUMPTION ***: This function assumes test_tasks_list contains tuples of
    (prefix, label, case_id) as generated by a modified get_task_data function.
    """
    all_embeddings = []
    all_labels = []
    all_case_ids = []  # <-- For contamination fix

    device = next(model.parameters()).device
    model.eval()

    try:
        # Check if data has 3 items (prefix, label, case_id)
        _ = test_tasks_list[0][2]
    except (IndexError, TypeError):
        print("\n" + "=" * 50)
        print("âŒ ERROR in _get_all_test_embeddings:")
        print("Test data does not contain case_ids.")
        print("Please modify get_task_data in data_generator.py to return:")
        print("(prefix, label, case_id) tuples.")
        print("Aborting retrieval-augmented evaluation.")
        print("=" * 50 + "\n")
        return None, None, None

    with torch.no_grad():
        for i in tqdm(range(0, len(test_tasks_list), batch_size), desc="Pre-computing test embeddings"):
            batch_tasks = test_tasks_list[i:i + batch_size]
            sequences = [t[0] for t in batch_tasks]
            labels = [t[1] for t in batch_tasks]
            case_ids = [t[2] for t in batch_tasks]  # <-- Get case_ids

            if not sequences: continue

            # Use the model's internal processing function
            encoded_batch = model._process_batch(sequences)

            all_embeddings.append(encoded_batch.cpu())
            all_labels.extend(labels)
            all_case_ids.extend(case_ids)  # <-- Store case_ids

    if not all_embeddings:
        return None, None, None

    all_embeddings_tensor = torch.cat(all_embeddings, dim=0).to(device)
    all_labels_tensor = torch.as_tensor(all_labels, device=device)
    all_case_ids_array = np.array(all_case_ids)  # Use numpy for easy string comparison

    return all_embeddings_tensor, all_labels_tensor, all_case_ids_array


def evaluate_retrieval_augmented(model, test_tasks, num_retrieval_k_list, num_test_queries=200):
    """
    Retrieval-Augmented evaluation.
    (Moved from testing.py)

    1. Computes all test embeddings.
    2. For each query, finds k-NN *from other cases* to form the support set.
    """
    print("\nðŸ”¬ Starting Retrieval-Augmented Evaluation...")
    model.eval()

    task_embeddings = {}

    # --- 1. Pre-compute all embeddings ---
    for task_type, task_data in test_tasks.items():
        if not task_data:
            print(f"Skipping {task_type}: No test data available.")
            continue

        # Get embeddings, labels, and case_ids
        embeddings, labels, case_ids = _get_all_test_embeddings(model, task_data)

        if embeddings is None:  # Error already printed in helper
            return

        # L2-normalize for efficient cosine similarity
        embeddings = F.normalize(embeddings, p=2, dim=1)
        task_embeddings[task_type] = (embeddings, labels, case_ids)  # Store all 3
        print(f"  - Pre-computed {embeddings.shape[0]} embeddings for {task_type}.")

    # --- 2. Evaluate using k-NN retrieval ---
    for task_type, (all_embeddings, all_labels, all_case_ids) in task_embeddings.items():
        print(f"\n--- Evaluating task: {task_type} ---")

        num_total_samples = all_embeddings.shape[0]
        if num_total_samples < 2:
            print("Skipping: Not enough samples to evaluate.")
            continue

        num_queries = min(num_test_queries, num_total_samples)
        query_indices = random.sample(range(num_total_samples), num_queries)

        for k in num_retrieval_k_list:
            # Need at least k+1 samples (1 query, k support) *from different cases*
            # This check is complex, so we'll just check if k < N
            if k >= num_total_samples:
                print(f"Skipping [k={k}]: k is larger than total samples.")
                continue

            all_preds, all_true_labels = [], []

            for query_idx in query_indices:
                query_embedding = all_embeddings[query_idx:query_idx + 1]  # [1, D]
                query_label = all_labels[query_idx]
                query_case_id = all_case_ids[query_idx]  # <-- Get query case_id

                # --- Find k-NN Support Set ---
                # Cosine similarity: (1, D) @ (D, N) -> (1, N)
                sims = query_embedding @ all_embeddings.T

                # --- CONTAMINATION FIX ---
                # Find all indices that share the same case ID as the query
                same_case_indices = np.where(all_case_ids == query_case_id)[0]

                # Mask out ALL samples from the same case (including the query itself)
                same_case_indices_tensor = torch.from_numpy(same_case_indices).to(sims.device)
                sims[0, same_case_indices_tensor] = -float('inf')
                # --- END FIX ---

                # Get top k most similar (now guaranteed to be from different cases)
                top_k_indices = torch.topk(sims.squeeze(0), k).indices

                support_embeddings = all_embeddings[top_k_indices]  # [k, D]
                support_labels = all_labels[top_k_indices]  # [k]

                # --- Get Prediction ---
                with torch.no_grad():
                    if task_type == 'classification':
                        logits, proto_classes = model.proto_head.forward_classification(
                            support_embeddings, support_labels, query_embedding
                        )

                        # --- "NO INVALID QUERIES" FIX ---
                        # Get the index of the top logit (e.g., 0, 1, 2...)
                        pred_label_idx = torch.argmax(logits, dim=1).item()

                        # Find the *actual* class label (e.g., "Activity C")
                        # that corresponds to this index
                        predicted_class_label = proto_classes[pred_label_idx].item()

                        all_preds.append(predicted_class_label)
                        all_true_labels.append(query_label.item())
                        # --- END FIX ---

                    else:  # Regression
                        prediction = model.proto_head.forward_regression(
                            support_embeddings, support_labels.float(), query_embedding
                        )
                        all_preds.append(prediction.item())
                        all_true_labels.append(query_label.item())

            if not all_true_labels: continue

            # --- 3. Report Metrics ---
            if task_type == 'classification':
                # No need to filter for -100 anymore
                print(
                    f"[{k}-NN] Retrieval Accuracy: {accuracy_score(all_true_labels, all_preds):.4f} (on {len(all_true_labels)} queries)")
            else:
                preds = inverse_transform_time(np.array(all_preds));
                preds[preds < 0] = 0
                labels = inverse_transform_time(np.array(all_true_labels))
                print(
                    f"[{k}-NN] Retrieval MAE: {mean_absolute_error(labels, preds):.4f} | R-squared: {r2_score(labels, preds):.4f}")
