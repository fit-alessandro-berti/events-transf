# File: evaluation/eval_retrieval.py
import torch
import torch.nn.functional as F
import random
import numpy as np
from sklearn.metrics import accuracy_score, mean_absolute_error, r2_score
from tqdm import tqdm

# --- Import from project files ---
from time_transf import inverse_transform_time
# ðŸ”» NEW IMPORT ðŸ”»
from utils.retrieval_utils import find_knn_indices


# ðŸ”º END NEW ðŸ”º


def _report_similarity_metrics(embeddings: torch.Tensor, labels: torch.Tensor, max_knn_queries=1000, knn_k_list=(5, 10)):
    """
    Reports quick, mean-aggregated similarity metrics for classification embeddings.
    Uses mean-centered cosine similarity to reduce anisotropy effects.
    """
    if embeddings.numel() == 0 or labels.numel() == 0:
        print("  - Similarity metrics: skipped (empty embeddings/labels).")
        return

    device = embeddings.device
    labels = labels.to(device)

    # Mean-center then re-normalize to reduce cosine anisotropy
    mean_emb = embeddings.mean(dim=0, keepdim=True)
    centered = embeddings - mean_emb
    centered = F.normalize(centered, p=2, dim=1)

    # Build centroids via scatter-add for speed
    unique_labels, inverse = torch.unique(labels, sorted=True, return_inverse=True)
    num_classes = unique_labels.numel()
    if num_classes < 2:
        print(f"  - Similarity metrics: skipped (num_classes={num_classes}).")
        return

    n, d = centered.shape
    centroids = torch.zeros((num_classes, d), device=device)
    counts = torch.zeros((num_classes,), device=device)
    centroids.scatter_add_(0, inverse[:, None].expand(-1, d), centered)
    counts.scatter_add_(0, inverse, torch.ones((n,), device=device))
    centroids = centroids / counts[:, None].clamp_min(1.0)
    centroids = F.normalize(centroids, p=2, dim=1)

    # 1) Intra-class cohesion: cosine to own centroid
    sims_to_own = (centered * centroids[inverse]).sum(dim=1)
    intra_mean = sims_to_own.mean().item()

    # 2) Inter-class centroid cosine (mean over pairs)
    centroid_sims = centroids @ centroids.T
    triu_idx = torch.triu_indices(num_classes, num_classes, offset=1, device=device)
    inter_mean = centroid_sims[triu_idx[0], triu_idx[1]].mean().item()

    # 3) Margin: sim to own centroid minus max sim to other centroids
    sims_all = centered @ centroids.T
    sims_all[torch.arange(n, device=device), inverse] = -float('inf')
    max_other = sims_all.max(dim=1).values
    margin_mean = (sims_to_own - max_other).mean().item()

    # 4) kNN purity (sampled for speed)
    knn_purity = {}
    max_queries = min(max_knn_queries, n)
    if max_queries < n:
        query_idx = torch.randperm(n, device=device)[:max_queries]
    else:
        query_idx = torch.arange(n, device=device)

    query_emb = centered[query_idx]
    query_labels = labels[query_idx]
    sims = query_emb @ centered.T
    sims[torch.arange(max_queries, device=device), query_idx] = -float('inf')

    for k in knn_k_list:
        k_eff = min(k, n - 1)
        if k_eff <= 0:
            knn_purity[k] = float('nan')
            continue
        topk_idx = torch.topk(sims, k_eff, dim=1).indices
        neighbor_labels = labels[topk_idx]
        purity = (neighbor_labels == query_labels[:, None]).float().mean().item()
        knn_purity[k] = purity

    print(
        "  - Similarity metrics (mean, mean-centered cosine): "
        f"intra_centroid_cos={intra_mean:.4f} | "
        f"inter_centroid_cos={inter_mean:.4f} | "
        f"centroid_margin={margin_mean:.4f} | "
        + " ".join([f"knn_purity@{k}={knn_purity[k]:.4f}" for k in knn_k_list])
    )


def _get_all_test_embeddings(model, test_tasks_list, batch_size=64):
    """
    Helper function to compute embeddings for all (prefix, label, case_id) tuples.
    (Moved from testing.py)

    *** ASSUMPTION ***: This function assumes test_tasks_list contains tuples of
    (prefix, label, case_id) as generated by a modified get_task_data function.

    MODIFIED for MoE:
    - Calls `model._process_batch`, which in `MoEModel` will return
      the *average* embedding from all experts.
    """
    all_embeddings = []
    all_labels = []
    all_case_ids = []  # <-- For contamination fix

    device = next(model.parameters()).device
    model.eval()

    try:
        # Check if data has 3 items (prefix, label, case_id)
        _ = test_tasks_list[0][2]
    except (IndexError, TypeError):
        print("\n" + "=" * 50)
        print("âŒ ERROR in _get_all_test_embeddings:")
        print("Test data does not contain case_ids.")
        print("Please modify get_task_data in data_generator.py to return:")
        print("(prefix, label, case_id) tuples.")
        print("Aborting retrieval-augmented evaluation.")
        print("=" * 50 + "\n")
        return None, None, None

    with torch.no_grad():
        for i in tqdm(range(0, len(test_tasks_list), batch_size), desc="Pre-computing test embeddings"):
            batch_tasks = test_tasks_list[i:i + batch_size]
            sequences = [t[0] for t in batch_tasks]
            labels = [t[1] for t in batch_tasks]
            case_ids = [t[2] for t in batch_tasks]  # <-- Get case_ids

            if not sequences: continue

            # Use the model's internal processing function
            # For MoEModel, this returns the average embedding
            encoded_batch = model._process_batch(sequences)

            all_embeddings.append(encoded_batch.cpu())
            all_labels.extend(labels)
            all_case_ids.extend(case_ids)  # <-- Store case_ids

    if not all_embeddings:
        return None, None, None

    all_embeddings_tensor = torch.cat(all_embeddings, dim=0).to(device)
    all_labels_tensor = torch.as_tensor(all_labels, device=device)
    all_case_ids_array = np.array(all_case_ids)  # Use numpy for easy string comparison

    return all_embeddings_tensor, all_labels_tensor, all_case_ids_array


def evaluate_retrieval_augmented(
    model,
    test_tasks,
    num_retrieval_k_list,
    num_test_queries=200,
    candidate_percentages=None
):
    """
    Retrieval-Augmented evaluation.
    (Moved from testing.py)

    1. Computes all test embeddings (uses avg. expert embedding for MoE).
    2. For each query, finds k-NN *from other cases* to form the support set.
    3. Uses the proto_head from the *first expert* to make predictions.

    candidate_percentages: list of percentages of the candidate pool to sample
    for k-NN (100 = full pool).
    """
    print("\nðŸ”¬ Starting Retrieval-Augmented Evaluation...")
    model.eval()

    if not candidate_percentages:
        candidate_percentages = [100]

    # --- ðŸ”» MoE Fix ðŸ”» ---
    # The retrieval eval is tightly coupled to the MetaLearner's proto_head.
    # For MoE, we'll use the *average embedding space* (from _get_all_test_embeddings)
    # and the *first expert's* proto_head for making the final k-NN prediction.
    # This is an approximation but maintains compatibility.
    proto_head_to_use = model.experts[0].proto_head
    if model.num_experts > 1:
        print("  - (MoE) Using avg. embedding space and Expert 0's proto_head for k-NN eval.")
    # --- ðŸ”º End MoE Fix ðŸ”º ---

    task_embeddings = {}

    # --- 1. Pre-compute all embeddings ---
    for task_type, task_data in test_tasks.items():
        if not task_data:
            print(f"Skipping {task_type}: No test data available.")
            continue

        # Get embeddings, labels, and case_ids
        embeddings, labels, case_ids = _get_all_test_embeddings(model, task_data)

        if embeddings is None:  # Error already printed in helper
            return

        # --- Quick sanity checks ---
        try:
            has_nan = torch.isnan(embeddings).any().item()
            all_finite = torch.isfinite(embeddings).all().item()
            print(f"  - Embedding sanity: has_nan={has_nan}, all_finite={all_finite}")
        except Exception as e:
            print(f"  - Embedding sanity check failed: {e}")

        if case_ids is not None:
            unique_cases, case_counts = np.unique(case_ids, return_counts=True)
            print(f"  - Case ID stats: unique_cases={len(unique_cases)}, total_tasks={len(case_ids)}")
            if len(unique_cases) > 0:
                top_k = min(5, len(unique_cases))
                top_idx = np.argsort(case_counts)[-top_k:][::-1]
                top_cases = [(unique_cases[i], int(case_counts[i])) for i in top_idx]
                print(f"  - Top case_id counts: {top_cases}")
        # --- End sanity checks ---

        # L2-normalize for efficient cosine similarity
        embeddings = F.normalize(embeddings, p=2, dim=1)

        # Report quick similarity metrics for classification only
        if task_type == 'classification':
            _report_similarity_metrics(embeddings, labels)

        task_embeddings[task_type] = (embeddings, labels, case_ids)  # Store all 3
        print(f"  - Pre-computed {embeddings.shape[0]} embeddings for {task_type}.")

    # --- 2. Evaluate using k-NN retrieval ---
    for task_type, (all_embeddings, all_labels, all_case_ids) in task_embeddings.items():
        print(f"\n--- Evaluating task: {task_type} ---")

        num_total_samples = all_embeddings.shape[0]
        if num_total_samples < 2:
            print("Skipping: Not enough samples to evaluate.")
            continue

        num_queries = min(num_test_queries, num_total_samples)
        query_indices = random.sample(range(num_total_samples), num_queries)

        for pct in candidate_percentages:
            print(f"\n  - Candidate pool sampling: {pct}%")

            # Sample a global candidate pool once per percentage for consistent k comparisons
            if pct >= 100:
                non_candidate_indices_tensor = None
            elif pct <= 0:
                non_candidate_indices_tensor = torch.arange(
                    num_total_samples,
                    device=all_embeddings.device,
                    dtype=torch.long
                )
            else:
                sample_size = int(np.ceil(num_total_samples * (pct / 100.0)))
                sample_size = max(1, min(sample_size, num_total_samples))
                if sample_size == num_total_samples:
                    non_candidate_indices_tensor = None
                else:
                    candidate_pool_indices = np.random.choice(
                        num_total_samples,
                        size=sample_size,
                        replace=False
                    )
                    mask_np = np.ones(num_total_samples, dtype=bool)
                    mask_np[candidate_pool_indices] = False
                    non_candidate_indices_tensor = torch.from_numpy(
                        np.where(mask_np)[0]
                    ).to(all_embeddings.device)
                print(f"  - Candidate pool size: {sample_size} / {num_total_samples}")

            for k in num_retrieval_k_list:
                # Need at least k+1 samples (1 query, k support) *from different cases*
                # This check is complex, so we'll just check if k < N
                if k >= num_total_samples:
                    print(f"Skipping [k={k} | pct={pct}%]: k is larger than total samples.")
                    continue

                # ðŸ”» MODIFIED: Store preds and confidences ðŸ”»
                all_preds, all_true_labels, all_confidences = [], [], []
                # ðŸ”º END MODIFIED ðŸ”º

                for query_idx in query_indices:
                    query_embedding = all_embeddings[query_idx:query_idx + 1]  # [1, D]
                    query_label = all_labels[query_idx]

                    query_case_id = all_case_ids[query_idx]
                    same_case_indices_np = np.where(all_case_ids == query_case_id)[0]

                    if non_candidate_indices_tensor is None:
                        if same_case_indices_np.size == 0:
                            mask_tensor = None
                        else:
                            mask_tensor = torch.from_numpy(same_case_indices_np).to(all_embeddings.device)
                    else:
                        if same_case_indices_np.size == 0:
                            mask_tensor = non_candidate_indices_tensor
                        else:
                            same_case_tensor = torch.from_numpy(same_case_indices_np).to(all_embeddings.device)
                            mask_tensor = torch.cat([non_candidate_indices_tensor, same_case_tensor])

                    # Use the new helper function
                    # Note: query_embedding and all_embeddings are already normalized
                    top_k_indices = find_knn_indices(
                        query_embedding,
                        all_embeddings,
                        k=k,
                        indices_to_mask=mask_tensor
                    )

                    if top_k_indices.numel() == 0:
                        continue  # Not enough valid support items found

                    # ðŸ”ºðŸ”ºðŸ”º END REFACTORED BLOCK ðŸ”ºðŸ”ºðŸ”º

                    support_embeddings = all_embeddings[top_k_indices]  # [k, D]
                    support_labels = all_labels[top_k_indices]  # [k]

                    # --- Get Prediction ---
                    with torch.no_grad():
                        if task_type == 'classification':
                            # --- ðŸ”» MoE Fix ðŸ”» ---
                            logits, proto_classes, confidence = proto_head_to_use.forward_classification(
                                support_embeddings, support_labels, query_embedding
                            )
                            # --- ðŸ”º End MoE Fix ðŸ”º ---

                            if logits is None: continue  # Should not happen if top_k_indices.numel() > 0

                            # --- "NO INVALID QUERIES" FIX ---
                            # Get the index of the top logit (e.g., 0, 1, 2...)
                            pred_label_idx = torch.argmax(logits, dim=1).item()

                            # ðŸ”» MODIFIED: Get confidence for the predicted class ðŸ”»
                            # Index [0, pred_label_idx] to handle [1,C] shape
                            pred_confidence = confidence[0, pred_label_idx].item()
                            # ðŸ”º END MODIFIED ðŸ”º

                            # Find the *actual* class label (e.g., "Activity C")
                            # that corresponds to this index
                            predicted_class_label = proto_classes[pred_label_idx].item()

                            all_preds.append(predicted_class_label)
                            all_true_labels.append(query_label.item())
                            # ðŸ”» MODIFIED: Store confidence ðŸ”»
                            all_confidences.append(pred_confidence)
                            # ðŸ”º END MODIFIED ðŸ”º
                            # --- END FIX ---

                        else:  # Regression
                            # --- ðŸ”» MoE Fix ðŸ”» ---
                            prediction, confidence = proto_head_to_use.forward_regression(
                                support_embeddings, support_labels.float(), query_embedding
                            )
                            # --- ðŸ”º End MoE Fix ðŸ”º ---

                            # ðŸ”» MODIFIED: Index [0] for single-item tensors ðŸ”»
                            all_preds.append(prediction[0].item())
                            all_true_labels.append(query_label.item())
                            # ðŸ”» MODIFIED: Store confidence ðŸ”»
                            all_confidences.append(confidence[0].item())
                            # ðŸ”º END MODIFIED ðŸ”º

                if not all_true_labels:
                    print(f"Skipping [k={k} | pct={pct}%]: no valid queries.")
                    continue

                # --- 3. Report Metrics ---
                if task_type == 'classification':
                    # ðŸ”» MODIFIED: Report confidence ðŸ”»
                    avg_conf = np.mean(all_confidences)
                    print(
                        f"[{k}-NN | pct={pct}%] Retrieval Accuracy: {accuracy_score(all_true_labels, all_preds):.4f} | Avg. Confidence: {avg_conf:.4f} (on {len(all_true_labels)} queries)")
                    # ðŸ”º END MODIFIED ðŸ”º
                else:
                    # ðŸ”» MODIFIED: Report confidence ðŸ”»
                    preds_np = np.array(all_preds)
                    labels_np = np.array(all_true_labels)
                    avg_conf = np.mean(all_confidences)

                    preds = inverse_transform_time(preds_np);
                    preds[preds < 0] = 0
                    labels = inverse_transform_time(labels_np)
                    print(
                        f"[{k}-NN | pct={pct}%] Retrieval MAE: {mean_absolute_error(labels, preds):.4f} | R-squared: {r2_score(labels, preds):.4f} | Avg. Confidence: {avg_conf:.4f}")
                    # ðŸ”º END MODIFIED ðŸ”º
